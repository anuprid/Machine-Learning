{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Classification with Logistic Regression and SVM\n",
    "\n",
    "Before we start, please put your name and CUID in following format\n",
    "\n",
    ": Firstname LASTNAME, #00000000   //   e.g. Nianyi LI, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer:**   \n",
    "Your NAME, #XXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Rules of the Project Submission\n",
    "\n",
    "Python 3 and [Matplotlib](https://matplotlib.org/) will be used throughout the semseter, so it is important to be familiar with them. It is strongly suggested to go through [Stanford CS231n](http://cs231n.github.io/python-numpy-tutorial/) and [CS228](https://github.com/kuleshov/cs228-material/blob/master/tutorials/python/cs228-python-tutorial.ipynb) for more detailed Python and numpy tutorials if you haven't had used Python before. \n",
    "\n",
    "In some cells and files you will see code blocks that look like this:\n",
    "\n",
    "```python\n",
    "##############################################################################\n",
    "#                    TODO: Write the equation for a line                     #\n",
    "##############################################################################\n",
    "pass\n",
    "##############################################################################\n",
    "#                              END OF YOUR CODE                              #\n",
    "##############################################################################\n",
    "```\n",
    "\n",
    "You should replace the `pass` statement with your own code and leave the blocks intact, like this:\n",
    "\n",
    "```python\n",
    "##############################################################################\n",
    "#                    TODO: Write the equation for a line                     #\n",
    "##############################################################################\n",
    "y = m * x + b\n",
    "##############################################################################\n",
    "#                              END OF YOUR CODE                              #\n",
    "##############################################################################\n",
    "```\n",
    "\n",
    "When completing the notebook, please adhere to the following rules:\n",
    "- Do not write or modify any code outside of code blocks\n",
    "- Follow the instruction of the project description carefully\n",
    "- Run all cells before submitting. <span style=\"color:red\">**You will only get credit for code that has been run!**.</span>\n",
    "\n",
    "The last point is extremely important and bears repeating:\n",
    "\n",
    "### We will not re-run your notebook -- <span style=\"color:red\">you will only get credit for cells that have been run</span>\n",
    "\n",
    "### File name\n",
    "Your Python program should be named **yourlastname_yourfirstname_P3.ipynb**, then zip it and upload to Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "For this project we will apply both **Logistic Regression** and **SVM** to predict whether capacitors from a fabrication plant pass quality control based (QC) on two different tests. To train your system and determine its reliability you have a set of 118 examples. The plot of these examples is show below where a red x is a capacitor that failed QC and the green circles represent capacitors that passed QC.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://nianyil.people.clemson.edu/CPSC_4430/P3_new.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two text files with the data is available on Canvas: a training set of 85 examples and a test set of 33 examples. Both are formatted as\n",
    "- First line: **m** and **n**, tab separated\n",
    "- Each line after that has two real numbers representing the results of the two tests, followed by a *1.0* if the capacitor *passed* QC and a *0.0* if it *failed* QC—tab separated.\n",
    "\n",
    "You need to write a code to read data from the file. You **can** use packages, such as **panda**, to load the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 85)\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#         TODO: Write the code for reading data from file                    #\n",
    "##############################################################################\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "x1, x2, y = np.loadtxt('P3train.txt', skiprows = 1, unpack = True)\n",
    "x1_test, x2_test, y_test = np.loadtxt('P3test.txt', skiprows = 1, unpack = True)\n",
    "#x = np.column_stack([np.ones(len(x1)),x1, np.power(x1, 2), x2, x1*x2, x1*np.power(x2, 2), np.power(x2, 2), np.power(x1, 2)* x2, np.power(x1, 2)*np.power(x2, 2)])\n",
    "#X = np.matrix(x)\n",
    "#print(x.shape)\n",
    "#print(X)\n",
    "X = np.column_stack([\n",
    "    np.ones(len(x1)),                   # Bias term (1 for each data point)\n",
    "    x1,                                 # x1\n",
    "    np.power(x1, 2),                    # x1^2\n",
    "    x2,                                 # x2\n",
    "    x1 * x2,                            # x1 * x2\n",
    "    x1 * np.power(x2, 2),               # x1 * x2^2\n",
    "    np.power(x2, 2),                    # x2^2\n",
    "    np.power(x1, 2) * x2,               # x1^2 * x2\n",
    "    np.power(x1, 2) * np.power(x2, 2)   # x1^2 * x2^2\n",
    "])\n",
    "\n",
    "X = X.T\n",
    "print(X.shape)\n",
    "##############################################################################\n",
    "#                              END OF YOUR CODE                              #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your assignment is to use what you have learned from the class slides and homework to create (**from scratch in Python**, not by using Logistic Regression library function!) a **Logistic Regression** and **SVM** binary classifier to predict whether each capacitor in the test set will pass QC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are free to use any model variation and any testing or training approach we have discussed for logistic regression. In particular, since this data is not linear, I assume you will want to add new features based on power of the original two features to create a good decision boundary. $w_0 + w_1x_1 + w_2x_2$ is not going to work!\n",
    "One choice might be\n",
    "- $\\textbf{w}^T \\textbf{x} = w_0 + w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + w_5x_5 +w_6x_6 + w_7x_7 + w_8x_8$    where the new features are created as follows:\n",
    "\n",
    "| New Features |From Original Features |\n",
    "| --- | --- |\n",
    "|$x_1$\t| $x_1$|\n",
    "|$x_2$\t| $x_1^2$|\n",
    "|$x_3$\t| $x_2$||\n",
    "|$x_4$\t| $x_1x_2$|\n",
    "|$x_5$\t| $x_1x_2^2$|\n",
    "|$x_6$\t| $x_2^2$|\n",
    "|$x_7$\t| $x_1^2x_2$|\n",
    "|$x_8$\t| $x_1^2x_2^2$|\n",
    "\n",
    "Note that it is easy to create a small Python program that reads in your  original features, uses a nested loop to create the new features and then writes them to a file:\n",
    "\n",
    "```python\n",
    "thePower = 2\n",
    "for j in range(thePower+1): \n",
    "    for i in range(thePower+1):\n",
    "        temp = (x1**i)*(x2**j)\n",
    "        if (temp != 1):\n",
    "            fout1.write(str(temp)+\"\\t\") fout1.write(str(y)+\"\\n\")\n",
    "```\n",
    "\n",
    "With a few additions to the code, you can make a program to create combinations of any powers of $x_1$ and $x_2$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#           TODO: Define the Logistic regression models                      #\n",
    "##############################################################################\n",
    "def logistic_model(x1, x2, w):\n",
    "    e_z = np.exp(-(compute_wTx(x1, x2, w)))\n",
    "    y_pred = 1 / (1 + (e_z))\n",
    "    return y_pred\n",
    "def compute_wTx(x1, x2, w):\n",
    "    wTx = (\n",
    "        w[0] + \n",
    "        w[1] * x1 + \n",
    "        w[2] * x1**2 + \n",
    "        w[3] * x2 + \n",
    "        w[4] * x1 * x2 + \n",
    "        w[5] * x1 * x2**2 + \n",
    "        w[6] * x2**2 + \n",
    "        w[7] * x2 * x1**2 + \n",
    "        w[8] * (x1**2) * (x2**2)\n",
    "    )\n",
    "    return wTx\n",
    "##############################################################################\n",
    "#                              END OF YOUR CODE                              #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization using Gradient Decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have defined the logistic regression model, you need to find the weights using the Gradient Decent algorithm. You need to implement the Vanilla Gradient Decent from scratch in Python.\n",
    "\n",
    "You need to specify the hyperparameters of GD, and plot the training loss curve (**J-curve**). The loss function should be the binary cross-entropy loss function that we introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#           TODO: Implement the Gradient Decent Algorithm                    #\n",
    "##############################################################################\n",
    "# Define the hyperparameters:\n",
    "# Numbers of epoch (epoch_num), learning rate (lr), and the initial weights(w)\n",
    "epoch_num = 1000\n",
    "lr = 0.1\n",
    "w = np.zeros(9)\n",
    "cost_history = []\n",
    "\n",
    "# Define the loss:\n",
    "def cross_entropy_loss(y_pred,y):\n",
    "    m = len(y)\n",
    "    J = -(1/m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    cost_history.append(J)\n",
    "    return J\n",
    "\n",
    "# Calculate the gradient function:\n",
    "def gradient_func(w,X,y,y_pred,x1, x2):\n",
    "    y_pred = logistic_model(x1, x2, w)\n",
    "    J = cross_entropy_loss(y_pred,y)\n",
    "    print(J)\n",
    "    #J.reshape(-1, 1)\n",
    "    #gradient_value = w - (lr * J * X)\n",
    "    #gradient_value = np.dot(X.T, J) / len(y)\n",
    "    error = y_pred - y\n",
    "    gradient_value = np.dot(X.T, error) / len(y)\n",
    "    return gradient_value\n",
    "    \n",
    "# Implement the Gradient decent algorithm using for loop\n",
    "def Vanilla_GD(epoch_num,lr,w,X,y,x1,x2):\n",
    "    for epoch in range(epoch_num):\n",
    "        gradient = gradient_func(w,X,y,y_pred,x1, x2)\n",
    "        w = w - lr * gradient\n",
    "    return w\n",
    "##############################################################################\n",
    "#                              END OF YOUR CODE                              #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, print out the final weights and plot the **J-curve/Loss curve** of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599454\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (85,9) and (85,) not aligned: 9 (dim 1) != 85 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m##############################################################################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#                     TODO: Plot the J curve                                 #\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m##############################################################################\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print out the final weights\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m w \u001b[38;5;241m=\u001b[39m Vanilla_GD(epoch_num,lr,w,X,y,x1,x2)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(w)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(cost_history)\n",
      "Cell \u001b[0;32mIn[156], line 33\u001b[0m, in \u001b[0;36mVanilla_GD\u001b[0;34m(epoch_num, lr, w, X, y, x1, x2)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mVanilla_GD\u001b[39m(epoch_num,lr,w,X,y,x1,x2):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_num):\n\u001b[0;32m---> 33\u001b[0m         gradient \u001b[38;5;241m=\u001b[39m gradient_func(w,X,y,y_pred,x1, x2)\n\u001b[1;32m     34\u001b[0m         w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m gradient\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m w\n",
      "Cell \u001b[0;32mIn[156], line 27\u001b[0m, in \u001b[0;36mgradient_func\u001b[0;34m(w, X, y, y_pred, x1, x2)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#J.reshape(-1, 1)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#gradient_value = w - (lr * J * X)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#gradient_value = np.dot(X.T, J) / len(y)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m error \u001b[38;5;241m=\u001b[39m y_pred \u001b[38;5;241m-\u001b[39m y\n\u001b[0;32m---> 27\u001b[0m gradient_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X\u001b[38;5;241m.\u001b[39mT, error) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gradient_value\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (85,9) and (85,) not aligned: 9 (dim 1) != 85 (dim 0)"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#                     TODO: Plot the J curve                                 #\n",
    "##############################################################################\n",
    "# Print out the final weights\n",
    "w = Vanilla_GD(epoch_num,lr,w,X,y,x1,x2)\n",
    "print(w)\n",
    "print(cost_history)\n",
    "# Plot the J curve w.r.t. the iteration numbers\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(epoch_num), cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('J-Curve / Loss Curve of Training')\n",
    "plt.show()\n",
    "##############################################################################\n",
    "#                              END OF YOUR CODE                              #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your data and plot, you should then briefly discuss how you can ensure that the model is well trained.\n",
    "\n",
    "**Your Answer:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance on testing set:\n",
    "- Print out the confusion matrix\n",
    "- Calculate and print out the *accuracy*, *precision*, *recall*, and *F1* value of your model\n",
    "\n",
    "**Note that:**\n",
    "- For **undergrads** *(CPSC 4430)* the final accuracy of both algorithms on your test set should be higher than  <span style=\"color:red\">**70%**</span>\n",
    "- For **graduate-level** *(CPSC 6430)* the final accuracy of both algorithms on your test set should be higher than  <span style=\"color:red\">**85%**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: nan\n",
      "Precision: 0\n",
      "Recall: 0\n",
      "F1 Score: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m2/frgc9fls66vgxdj3c5ywxjkr0000gn/T/ipykernel_70873/696122897.py:10: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  accuracy = (tp + tn) / (tp + tn + fp + fn)\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "#                           TODO: Model Evaluation                           #\n",
    "##############################################################################\n",
    "def model_evaluation(y_test, y_t_pred):\n",
    "    tp = np.sum((y_test == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_test == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_test == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_test == 1) & (y_pred == 0))\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision * recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return accuracy, precision, recall, f1_score\n",
    "    \n",
    "y_t_pred = logistic_model(x1_test, x2_test, w)\n",
    "\n",
    "accuracy, precision, recall, f1_score = model_evaluation(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "##############################################################################\n",
    "#                              END OF YOUR CODE                              #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you need to use the previous training and testing data file. \n",
    "\n",
    "You are **allowed** to use the svm functions in the **Scikit-learn** library and don’t need to implement the algorithm from scratch.\n",
    "\n",
    "- You need to try at least **three** different kernel functions of SVM, and pick the **best** model.\n",
    "- You need to print out the final weights got from your best SVM model.\n",
    "\n",
    "**Note that:**\n",
    "- For **undergrads** *(CPSC 4430)* the final accuracy of both algorithms on your test set should be higher than  <span style=\"color:red\">**70%**</span>\n",
    "- For **graduate-level** *(CPSC 6430)* the final accuracy of both algorithms on your test set should be higher than  <span style=\"color:red\">**85%**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                      TODO: Classfication using SVM                         #\n",
    "##############################################################################\n",
    "# Pick the best model\n",
    "pass\n",
    "\n",
    "# Print out the final weights\n",
    "pass\n",
    "##############################################################################\n",
    "#                              END OF YOUR CODE                              #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Decision Boundary and Model Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to plot the decision boundary of Logistic Regression and SVM that you previously trained separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#                   TODO: Plot the Decision Boundary                         #\n",
    "##############################################################################\n",
    "pass\n",
    "##############################################################################\n",
    "#                              END OF YOUR CODE                              #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your data and plot, you should then briefly discuss which one has better performance and why.\n",
    "\n",
    "**Your Answer:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
